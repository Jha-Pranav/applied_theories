{
 "cells": [
  {
   "cell_type": "raw",
   "id": "251ac377",
   "metadata": {},
   "source": [
    "Normalization ==> Pre-tokenization ==> Model ==> Trainer ==> Post-Processing ==> Decoders\n",
    "\n",
    "https://huggingface.co/docs/tokenizers/python/latest/components.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f958977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "76f03737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0,len(dataset),1000):\n",
    "        yield dataset[i:i+1000]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dba06eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers, pre_tokenizers, models, trainers,processors,decoders, Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a30003",
   "metadata": {},
   "source": [
    "#### Building a WordPiece tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5f8ec78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b3f21b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token='<unknown>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ac24e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3906ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(),normalizers.Lowercase(),normalizers.StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2a801ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are u?'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\")  ## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "04b4eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre Tokenizer \n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5f398a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d88a172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5ad3b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Whitespace(),pre_tokenizers.Punctuation()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e8c76dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5484f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2bd22abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<unknown>\", \"<padding>\", \"<start>\", \"<sep>\", \"<mask>\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000,special_tokens=special_tokens,continuing_subword_prefix=\"<ss>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d53f64fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(),trainer=trainer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6757c0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'pra', '<ss>na', '<ss>v', 'j', '<ss>ha', ',', 'i', 'live', 'in', 'bang', '<ss>lore']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode('This is Pranav Jha, I live in Banglore')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522b474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "285a034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "12a7d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "start_token_id = tokenizer.token_to_id('<start>')\n",
    "sep_token_id = tokenizer.token_to_id('<sep>')\n",
    "print(start_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "612ab10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "single= f\"<start>:0 $A:0 <sep>:0\",\n",
    "pair=  f\"<start>:0 $A:0 <sep>:0 $B:1 <sep>:1\",\n",
    "special_tokens= [('<start>',start_token_id),('<sep>',sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e9e0a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'this', 'is', 'pra', '<ss>na', '<ss>v', 'j', '<ss>ha', ',', 'i', 'live', 'in', 'bang', '<ss>lore', '<sep>', 'i', 'work', 'in', 'ericsson', '<sep>']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode('This is Pranav Jha, I live in Banglore',\"I work in Ericsson\")\n",
    "\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8d44d4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9a9d737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix='<ss>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "08b148c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is pranav jha, i live in banglore i work in ericsson'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "acbe89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ddf5a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload \n",
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b1bfd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer \n",
    "\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b83724fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "#     tokenizer_object=tokenizer,\n",
    "    tokenizer_file = \"tokenizer.json\",    unk_token=\"<unknown>\",\n",
    "    pad_token=\"<padding>\",\n",
    "    cls_token=\"<start>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "824bb861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=25000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '<unknown>', 'sep_token': '<sep>', 'pad_token': '<padding>', 'cls_token': '<start>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f8725ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c87b1",
   "metadata": {},
   "source": [
    "#### Building a BPE tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "85a57d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers,pre_tokenizers,models,trainers,processors,Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a67f2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "tokenizer = Tokenizer(model= models.BPE(unk_token='<unknown>',continuing_subword_prefix='<sw>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "369cb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize  -- > GPT2 DOES NOT NEED NORMALIZER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1b5caedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('Ġis', (4, 7)),\n",
       " ('ĠPranav', (7, 14)),\n",
       " (',', (14, 15)),\n",
       " ('ĠI', (15, 17)),\n",
       " ('Ġam', (17, 20)),\n",
       " ('Ġlearning', (20, 29)),\n",
       " ('ĠByteLevel', (29, 39)),\n",
       " ('Ġtokenization', (39, 52)),\n",
       " ('Ġfrom', (52, 57)),\n",
       " ('Ġhugging', (57, 65)),\n",
       " ('Ġface', (65, 70))]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PRE TOKENIZER\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str('This is Pranav, I am learning ByteLevel tokenization from hugging face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b78ba7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainer\n",
    "trainer = trainers.BpeTrainer(vocab_size=25000,special_tokens=[\"<|endoftext|>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8919fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(),trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "63938e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"This is a sample text to experiment with the gpt-2 tekenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "11934f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġsample', 'Ġtext', 'Ġto', 'Ġexperiment', 'Ġwith', 'Ġthe', 'Ġg', 'pt', '-', '2', 'Ġte', 'ken', 'izer']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7623d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing\n",
    "tokenizer.post_process = processors.ByteLevel(trim_offsets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "852df871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġsample', 'Ġtext', 'Ġto', 'Ġexperiment', 'Ġwith', 'Ġthe', 'Ġg', 'pt', '-', '2', 'Ġte', 'ken', 'izer']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a sample text to experiment with the gpt-2 tekenizer\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f7dfc067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' with'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start,end = encoding.offsets[9]\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4f170783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample text to experiment with the gpt-2 tekenizer'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## decoder \n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "94a55211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")\n",
    "\n",
    "#    --------------- or ---------------------\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338eac08",
   "metadata": {},
   "source": [
    "#### Building a Unigram tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d5e63ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3cf13bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"▁Let's\", (0, 5)),\n",
       " ('▁test', (5, 10)),\n",
       " ('▁the', (10, 14)),\n",
       " ('▁pre-tokenizer!', (14, 29))]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e1a797fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    ")\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "634aaf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2ff616c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "913452f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d07da3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e3d726c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e0b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-deeplearning",
   "language": "python",
   "name": "applied-deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
