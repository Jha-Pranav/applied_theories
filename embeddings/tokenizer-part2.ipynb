{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c671e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9f5da4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaa5930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"My name is Pranav Jha. I love coding\"\n",
    "encoding = tokenizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de347e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'P', '##rana', '##v', 'J', '##ha', '.', 'I', 'love', 'coding', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cd215f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 4, 4, 5, 6, 7, 8, None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "469245c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.sequence_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b0501db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharSpan(start=11, end=17)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_to_chars(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b1af290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pranav'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[11:17]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3ebba35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 16),\n",
       " (16, 17),\n",
       " (18, 19),\n",
       " (19, 21),\n",
       " (21, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 36),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "617ce707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796021,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9466cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForTokenClassification\n",
    "checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint,)\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a4da5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = tokenizer(example,return_tensors=\"pt\")\n",
    "output = model(**encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "594ebbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 9])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5d481dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = model.config.id2label\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77e16c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output.logits, dim=-1)[0].tolist()\n",
    "predictions = output.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb3118dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9993104934692383, 'word': 'P'}, {'entity': 'I-PER', 'score': 0.9922254085540771, 'word': '##rana'}, {'entity': 'I-PER', 'score': 0.9980796575546265, 'word': '##v'}, {'entity': 'I-PER', 'score': 0.999276340007782, 'word': 'J'}, {'entity': 'I-PER', 'score': 0.9982074499130249, 'word': '##ha'}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "tokens = encode.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a70b5787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 16),\n",
       " (16, 17),\n",
       " (18, 19),\n",
       " (19, 21),\n",
       " (21, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 36),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77c58eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rana'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[12:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d14f0d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'P', '##rana', '##v', 'J', '##ha', '.', 'I', 'love', 'coding', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encode.input_ids.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "572ea2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9974198698997497, 'word': 'Pranav Jha', 'start': 11, 'end': 21}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2ace33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pranav Jha'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[11:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2620d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fast tokenizers in the QA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09d463bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9802601933479309,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8ddb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e995f29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-cased-distilled-squad', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d721b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length :  40\n",
      "Tokens Length :  56\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "print(\"Context length : \",len(context.split()))\n",
    "print(\"Tokens Length : \",len(tokenizer.tokenize(context)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "303e4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(**tokenizer(question,context,return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57c4512a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.start_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e258a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c343e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Length :  608\n",
      "Tokens Length :  868\n"
     ]
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "🤗 Transformers: State of the Art NLP\n",
    "\n",
    "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "Hugging Face is a technology company based in New York City. It is known for its contributions to the field of natural language processing (NLP) and its open-source library, Transformers. The Transformers library provides pre-trained models that have achieved state-of-the-art performance on various NLP tasks, such as text classification, language generation, and named entity recognition.\n",
    "\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The company has gained significant recognition for democratizing access to powerful NLP models through its user-friendly API and easy-to-use Python library. Developers and researchers around the world use Hugging Face's models to build a wide range of applications, from chatbots to language translation systems.\n",
    "\n",
    "The Transformers library includes a diverse set of models, each designed for specific NLP tasks. Some of the popular models in the library include BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), RoBERTa, and T5 (Text-to-Text Transfer Transformer). These models have been fine-tuned on large datasets and can be further fine-tuned on domain-specific data to achieve even better performance.\n",
    "\n",
    "Hugging Face's commitment to open-source and collaboration has made it a central hub for the NLP community. The library has a vast user base, and the company actively engages with developers and researchers through its forums, tutorials, and educational resources. Hugging Face continues to push the boundaries of NLP research and innovation, making it a leading force in the field.\n",
    "\n",
    "The company's headquarters in New York City serves as a hub for its operations, with a dedicated team of experts working on improving the library, developing new models, and exploring the latest advancements in NLP. Hugging Face's impact on the NLP landscape is undeniable, and it continues to shape the future of language technology.\n",
    "\n",
    "with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow.\n",
    "\"\"\"\n",
    "print(\"Context Length : \",len(long_context.split()))\n",
    "print(\"Tokens Length : \",len(tokenizer.tokenize(long_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ba732037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9420106410980225,\n",
       " 'start': 3998,\n",
       " 'end': 4025,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8a2eb256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Length :  4027\n"
     ]
    }
   ],
   "source": [
    "print(\"Context Length : \",len(long_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "92355943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753424"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "868 * 868"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6c8d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(**tokenizer(question, context,return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecabd159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.4952, -6.4454, -4.7115, -7.0968, -7.0726, -7.4981, -5.5397, -4.1368,\n",
       "         -5.9199, -5.4193, -1.5920, -1.0857, -5.0981, -2.9331, -3.4070,  2.2467,\n",
       "          5.1563, -1.3602, -2.2209, -0.9686, -4.8112, -2.2527,  1.4383, 10.1211,\n",
       "         -1.5311,  2.2685, -1.8952, -2.2108, -4.2142, -2.5571, -2.3252, -2.6046,\n",
       "          1.7047, -1.9867, -1.7211, -0.5415, -2.0239, -4.4246, -5.1012, -4.4966,\n",
       "         -7.8940, -6.7200, -4.6759, -6.3279, -4.8339, -5.1839, -3.3724, -7.4120,\n",
       "         -8.1542, -4.4871, -7.4659, -4.3293, -4.2293, -3.1903, -7.9467, -5.2665,\n",
       "         -7.5902, -5.0570, -7.4476, -7.9083, -6.5951, -7.4061, -8.8821, -7.6749,\n",
       "         -6.9879, -7.0466, -5.4193]], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e8843551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c93953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=model.config.max_position_embeddings,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "405a84bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Which', 'deep', 'learning', 'libraries', 'back', '[UNK]', 'Transformers', '?', '[SEP]', '[UNK]', 'Transformers', ':', 'State', 'of', 'the', 'Art', 'NL', '##P', '[UNK]', 'Transformers', 'provides', 'thousands', 'of', 'pre', '##tra', '##ined', 'models', 'to', 'perform', 'tasks', 'on', 'texts', 'such', 'as', 'classification', ',', 'information', 'extraction', ',', 'question', 'answering', ',', 'sum', '##mar', '##ization', ',', 'translation', ',', 'text', 'generation', 'and', 'more', 'in', 'over', '100', 'languages', '.', 'Its', 'aim', 'is', 'to', 'make', 'cutting', '-', 'edge', 'NL', '##P', 'easier', 'to', 'use', 'for', 'everyone', '.', '[UNK]', 'Transformers', 'provides', 'API', '##s', 'to', 'quickly', 'download', 'and', 'use', 'those', 'pre', '##tra', '##ined', 'models', 'on', 'a', 'given', 'text', ',', 'fine', '-', 'tune', 'them', 'on', 'your', 'own', 'data', '##sets', 'and', 'then', 'share', 'them', 'with', 'the', 'community', 'on', 'our', 'model', 'hub', '.', 'At', 'the', 'same', 'time', ',', 'each', 'p', '##yt', '##hon', 'module', 'defining', 'an', 'architecture', 'is', 'fully', 'stand', '##alo', '##ne', 'and', 'can', 'be', 'modified', 'to', 'enable', 'quick', 'research', 'experiments', '.', 'Why', 'should', 'I', 'use', 'transform', '##ers', '?', '1', '.', 'Easy', '-', 'to', '-', 'use', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', ':', '-', 'High', 'performance', 'on', 'NL', '##U', 'and', 'NL', '##G', 'tasks', '.', '-', 'Low', 'barrier', 'to', 'entry', 'for', 'educators', 'and', 'practitioners', '.', '-', 'Few', 'user', '-', 'facing', 'abstract', '##ions', 'with', 'just', 'three', 'classes', 'to', 'learn', '.', '-', 'A', 'unified', 'API', 'for', 'using', 'all', 'our', 'pre', '##tra', '##ined', 'models', '.', '-', 'Lower', 'com', '##pute', 'costs', ',', 'smaller', 'carbon', 'foot', '##print', ':', '2', '.', 'Researchers', 'can', 'share', 'trained', 'models', 'instead', 'of', 'always', 're', '##tra', '##ining', '.', '-', 'P', '##rac', '##ti', '##tion', '##ers', 'can', 'reduce', 'com', '##pute', 'time', 'and', 'production', 'costs', '.', '-', 'Do', '##zen', '##s', 'of', 'architecture', '##s', 'with', 'over', '10', ',', '000', 'pre', '##tra', '##ined', 'models', ',', 'some', 'in', 'more', 'than', '100', 'languages', '.', '3', '.', 'Cho', '##ose', 'the', 'right', 'framework', 'for', 'every', 'part', 'of', 'a', 'model', \"'\", 's', 'lifetime', ':', '-', 'Train', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'in', '3', 'lines', 'of', 'code', '.', '-', 'Move', 'a', 'single', 'model', 'between', 'T', '##F', '##2', '.', '0', '/', 'P', '##y', '##T', '##or', '##ch', 'framework', '##s', 'at', 'will', '.', '-', 'Sea', '##m', '##lessly', 'pick', 'the', 'right', 'framework', 'for', 'training', ',', 'evaluation', 'and', 'production', '.', '4', '.', 'E', '##asily', 'custom', '##ize', 'a', 'model', 'or', 'an', 'example', 'to', 'your', 'needs', ':', '-', 'We', 'provide', 'examples', 'for', 'each', 'architecture', 'to', 'reproduce', 'the', 'results', 'published', 'by', 'its', 'original', 'authors', '.', '-', 'Model', 'internal', '##s', 'are', 'exposed', 'as', 'consistently', 'as', 'possible', '.', '-', 'Model', 'files', 'can', 'be', 'used', 'independently', 'of', 'the', 'library', 'for', 'quick', 'experiments', '.', 'Hu', '##gging', 'Face', 'is', 'a', 'technology', 'company', 'based', 'in', 'New', 'York', 'City', '.', 'It', 'is', 'known', 'for', 'its', 'contributions', 'to', 'the', 'field', 'of', 'natural', 'language', 'processing', '(', 'NL', '##P', ')', 'and', 'its', 'open', '-', 'source', 'library', ',', 'Transformers', '.', 'The', 'Transformers', 'library', 'provides', 'pre', '-', 'trained', 'models', 'that', 'have', 'achieved', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'various', 'NL', '##P', 'tasks', ',', 'such', 'as', 'text', 'classification', ',', 'language', 'generation', ',', 'and', 'named', 'entity', 'recognition', '.', 'Hu', '##gging', 'Face', 'was', 'founded', 'in', '2016', 'by', 'C', '##lé', '##ment', 'Del', '##ang', '##ue', ',', 'Julien', 'Cha', '##um', '##ond', ',', 'and', 'Thomas', 'Wolf', '.', 'The', 'company', 'has', 'gained', 'significant', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bfe479ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.sequence_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0d904705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512]) torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b5b5bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ffe39454",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d5d8ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ee0515e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "582dac84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "15201750",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.triu(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c80ed31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9c3e1cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5692, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[start_index,end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0269e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "60dd02fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 25), (44, 51))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets[start_index],offsets[end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ff0845be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5d1863a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e2d6179a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e7a5692b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "67910573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', (0, 3)), ('are', (4, 7)), ('you', (10, 13)), ('?', (14, 15))]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str('How are   you ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0639d93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('ĠĠ', (14, 16)),\n",
       " ('Ġyou', (16, 20)),\n",
       " ('?', (20, 21))]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are   you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "19a6be7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e351b454fa0441f8b5d2e5c7ebadf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472739cb774b42ff8402e2757ec63f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81f8195e3ca46d8a2de2042ea3320a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ec62e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Byte Pair Encoding Tokenizer - Implementing BPE\n",
    "\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26a6f517",
   "metadata": {},
   "source": [
    "Normalization\n",
    "Pre-tokenization\n",
    "Splitting the words into individual characters\n",
    "Applying the merge rules learned in order on those splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "97f10395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbba991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-deeplearning",
   "language": "python",
   "name": "applied-deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
