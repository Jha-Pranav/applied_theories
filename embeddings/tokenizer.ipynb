{
 "cells": [
  {
   "cell_type": "raw",
   "id": "74ef6128",
   "metadata": {},
   "source": [
    "# Training a new tokenizer from an old one\n",
    "\n",
    "# Data : This dataset was created for the CodeSearchNet challenge and contains millions of functions from open source libraries on GitHub in several programming languages. Here, we will load the Python part of this dataset:\n",
    "    # Url : https://wandb.ai/github/CodeSearchNet/benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66015b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce75838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def findspans(self, type,set=None):\n",
      "        \"\"\"Find span annotation of the specified type that include this word\"\"\"\n",
      "        if issubclass(type, AbstractAnnotationLayer):\n",
      "            layerclass = type\n",
      "        else:\n",
      "            layerclass = ANNOTATIONTYPE2LAYERCLASS[type.ANNOTATIONTYPE]\n",
      "        e = self\n",
      "        while True:\n",
      "            if not e.parent: break\n",
      "            e = e.parent\n",
      "            for layer in e.select(layerclass,set,False):\n",
      "                for e2 in layer:\n",
      "                    if isinstance(e2, AbstractSpanAnnotation):\n",
      "                        if self in e2.wrefs():\n",
      "                            yield e2\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'][90]['whole_func_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42052f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59af1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"gpt2\"\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d62037c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "print(old_tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948e5668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aecb3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "embedding = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876784eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained(\"./checkpoints/\")\n",
    "embedding = tokenizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fce1e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharSpan(start=15, end=16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.word_to_chars(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd1968f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'('"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[15:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a4216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-deeplearning",
   "language": "python",
   "name": "applied-deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
